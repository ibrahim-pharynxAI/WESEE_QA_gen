## 2.1 Theoretical framework

The CNN architecture has been pivotal in advancing the field of image recognition due to its ability to learn hierarchical feature representations. At the heart of these advancements are pooling layers, which reduce the spatial dimensions of the feature maps, thus decreasing computational complexity and enhancing the network's ability to capture dominant features. In this study, we explore the integration of three distinct pooling strategies, namely MaxPooling, MinPooling, and MaxMinPooling, each contributing uniquely to the robustness and accuracy of image classification tasks. This study elucidates the rationale behind the choice of these specific pooling layers and their collective impact on the proposed CNN model's performance.

- MaxPooling: MaxPooling is a standard pooling operation used in CNNs to down-sample the input's spatial dimensions by taking the maximum value over a specified window for each channel independently. This operation is instrumental in achieving translation invariance, reducing the sensitivity of the output to minor changes and shifts in the input image. By focusing on the most prominent features within each window, MaxPooling, along with positive ReLU, helps in accentuating features that are crucial for discrimination, thus improving the model's generalization capability [38, 39]. However, while effective in highlighting the strongest features, MaxPooling may overlook subtler, yet equally important, features present in the input.
- MinPooling: In contrast to MaxPooling, MinPooling operates by selecting the minimum value within each pooling window. This approach is less common in traditional CNN architectures but offers unique advantages, particularly in emphasizing low-intensity features that may be overshadowed by more dominant features in the MaxPooling process. By capturing these minimal values, MinPooling along with negative ReLU function can enhance the model's sensitivity to finer and more subtle patterns in the input data, which are often crucial in complex classification tasks, such as distinguishing between similar classes or identifying pathologies in medical images.
- MaxMinPooling: MaxMinPooling is a novel pooling strategy that aims to combine the strengths of MaxPooling and MinPooling by concurrently capturing both the maximum and minimum values within each pooling window. This dual approach allows the model to retain both the most and least activated features within the input feature map, offering a more comprehensive representation of the input's characteristics. The

integration of MaxMinPooling is motivated by the hypothesis that a richer feature set, encompassing both extremes of the feature activation spectrum, can significantly enhance the model's discrimination power, especially in tasks requiring fine-grained differentiation between classes.

The interleaved MaxPooling and MinPooling operations are structured such that each MaxPooling output is immediately followed by its corresponding MinPooling output. This configuration ensures an intricate balance and detailed preservation of feature information, allowing the network to capture both high- and low-intensity features of the input. Theoretical analysis suggests that this interleaving can improve gradient flow in deep networks by providing additional gradients during backpropagation [40-42]. We empirically validate this hypothesis through a series of experiments on benchmark datasets, demonstrating improved classification accuracy, particularly in scenarios where feature contrasts are critical to performance.
